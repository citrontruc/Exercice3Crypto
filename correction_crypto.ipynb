{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3\n",
    "\n",
    "---\n",
    "\n",
    "Le but de ce notebook est d'étudier le jeu de données de la valeur des crypto afin de pouvoir faire une prédiction des gains possibles qu'on pourrait avoir.\n",
    "\n",
    "Pour des raisons de taille, le jeu de données n'a pas pu être uploadé avec le github. Il faudra donc le récupérer à [l'adresse suivante](https://www.kaggle.com/c/g-research-crypto-forecasting/data). Seul les jeux de données train.csv et asset_details.csv nous intéressent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import dateutil.tz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des tables dans l'espace de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_train = pd.read_csv(\"g-research-crypto-forecasting/train.csv\")\n",
    "# supplemental_train est une donnée d'entrainement fournie pour indication. Ne pas utiliser.\n",
    "#crypto_additional_train = pd.read_csv(\"g-research-crypto-forecasting/supplemental_train.csv\")\n",
    "crypto_info = pd.read_csv(\"g-research-crypto-forecasting/asset_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piège : la colonne timestamp possède une valeur assez peu lisible.\n",
    "crypto_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 - Qualité des données\n",
    "\n",
    "1) Trouvez le nombre de lignes et de colonnes.\n",
    "1) Faites un join avec la table des infos pour restituer à chaque cryptomonnaie son nom.\n",
    "1) Trouvez le nombre de valeur manquantes. Quelle est la crypto-monnaie avec le plus de valeurs manquantes ?\n",
    "1) Créer une colonne date qui change la colonne timestamp en une vraie date.\n",
    "1) Timestamp maximal, minimal et nombre de points pour chacun de nos assets. Quelle semble être la granularité de notre timeseries ?\n",
    "1) Pour chacune des crypto-monnaies, de combien de points disposons-nous ? Que pouvons-nous dire en comparant avec la date minimal pour chacune de nos cryptomonnaies ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1)\n",
    "print(f\"Nous possédons {len(crypto_train)} lignes de données et {len(crypto_train.columns)} colonnes. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2) On fusionne les deux tables. Dans notre cas, on fait un left join sur la colonne commune.\n",
    "crypto_with_name = pd.merge(crypto_train, crypto_info, on=\"Asset_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3) trouvons le nombre de valeurs manquantes.\n",
    "\n",
    "# Il est déconseillé d'utiliser la commande .describe() car nous allons alors effectuer des calculs de moyennes et autres variables.\n",
    "# Dans notre cas, notre dataset est trop grand pour calculer tout ça.\n",
    "# crypto_train.describe()\n",
    "\n",
    "# A la place, nous allons nous borner à faire un calcul simple. Par colonne, nous disposons du nombre suivant de valeurs nulles.\n",
    "crypto_with_name.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constate que la majorité des valeurs nulles proviennent de la colonne target. Dans le cas des Timestamp, avoir des valeurs nulles est très grave.\n",
    "# En effet, on coupe la continuité du temps et comme le rapport de chaque variable avec les précédentes est important, on se prive d'une information capitale.\n",
    "\n",
    "# Nous séparons le nombre de valeurs nulles par crypto\n",
    "crypto_with_name.loc[crypto_with_name[\"Target\"].isna()].groupby(\"Asset_Name\").count()[\"timestamp\"].sort_values(ascending=False)\n",
    "# Le fait que les monnaies les plus connues soient les plus documentées nous semble pertinent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4) On utilise la librairie datetime qui possède une bibliothèque associée.\n",
    "tz_keep = dateutil.tz.tzutc()\n",
    "crypto_with_name[\"datetime\"] = crypto_with_name[\"timestamp\"].apply(lambda x : datetime.fromtimestamp(x, tz=tz_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5) On peut appliquer les méthodes min et max aux dates ==> Pas de soucis.\n",
    "print(f\"Les extremums des dates sont {crypto_with_name['datetime'].min()} pour le minimum et {crypto_with_name['datetime'].max()} pour le maximum.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En regardant les valeurs classées, on devine que la granularité des données est une minute.\n",
    "sorted(crypto_with_name['datetime'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6) On compte le nombre de points de données que l'on possède pour chacune de nos cryptomonnaies.\n",
    "crypto_with_name.groupby('Asset_Name')['timestamp'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On possède beaucoup de valeurs depuis le premier janvier 2018 MAIS pour ces monnaies, on ne possède pas le même nombre de points.\n",
    "# Il doit manquer des points intermédiaires ==> Ouch\n",
    "crypto_with_name.groupby('Asset_Name')['datetime'].min().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On possède toutes les monnaies jusqu'au 21 septembre 2021. On va effectivement avoir des valeurs manquantes au milieu.\n",
    "crypto_with_name.groupby('Asset_Name')['datetime'].max().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_with_name.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 - Analyse données\n",
    "\n",
    "---\n",
    "\n",
    "## Explication des valeurs des colonnes.\n",
    "\n",
    "- **timestamp** : La minute considérée par la colonne.\n",
    "- **Asset_ID** & **Asset_Name** : identifiant de la crypto-monnaie.\n",
    "- **Count** : Nombre d'échanges de la monnaie lors de la minute.\n",
    "- **Open** & **Close** : Prix d'ouverture et de fermeture (prix au début et à la fin de la minute).\n",
    "- **High** & **Low** : Prix maximal et minimal atteint dans la minute.\n",
    "- **Volume** : Montant total échangé lors de la minute.\n",
    "- **VWAP** : Prix moyen sur la minute pondéré par volume de vente.\n",
    "- **Target** : Log du retour sur investissement sur les 15 prochaines minutes auquel on a appliqué une transformation. f(log(prix à l'instant t+16) - log(prix à l'instant t+1))\n",
    "- **Weight** : Lié à la cryptomonnaie. Importance de la crypto-monnaie (valeur arbitraire).\n",
    "\n",
    "---\n",
    "\n",
    "Nous n'allons pas utiliser les valeurs de weight et target. Nous allons essayer de prédire la VWAP de la monnaie avec 7j d'avance.\n",
    "\n",
    "---\n",
    "\n",
    "1) Calculez la corrélations entre les valeurs moyennes des différentes monnaies sur l'année 2021. Ce résultat vous étonne-t-il ?\n",
    "1) Au vue de la question précédente, nous allons nous intéresser uniquement au bitcoin pour l'instant. Identifiez les timestamp manquantes pour le bitcoin.\n",
    "1) Quand sont atteints les optimums et pourquoi ? Quand sont atteint les plus grands écarts min max en valeur puis en %\n",
    "1) Quels jours de la semaine, quels mois de l'année et quelles semaines de l'années y a-t-il le plus de transactions (en moyenne) ? Décomposez les statistiques selon les périodes qui vous semblent pertinentes.\n",
    "1) Faire un graphe qui présente par semaine le prix moyen des cryptomonnaies avec leur min et le max (dans le même graphe).\n",
    "1) Autocorrelation décalée pour voir s'il y a des périodicités.\n",
    "1) Faire des graphe qui présentent la tendance et  la saisonnalité des données.\n",
    "1) Quelle méthode utiliseriez-vous pour faire jeux de test et train ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1) On commence par isoler l'année 2021\n",
    "crypto_2021 = crypto_with_name.loc[crypto_with_name[\"datetime\"].dt.year == 2021]\n",
    "# Afin de faire pivoter la ligne des Asset_Name et la transformer en colonne, on utilise la commande pivot\n",
    "pivoted_table = crypto_2021.pivot(index='datetime', columns='Asset_Name', values='VWAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constate que le coefficient de corrélation entre les différentes monnaies est assez fort.\n",
    "# Plusieurs monnaies étant liées, cela semble logique.\n",
    "\n",
    "matrix = pivoted_table.corr().round(2)\n",
    "sns.heatmap(matrix, annot=True, vmax=1, vmin=0, center=0, cmap='vlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2) Afin de trouver s'il manque des minutes sur notre timestamp, nous allons construire un dataset qui contient toutes les position (minute par minute) et le comparer à notre table crypto.\n",
    "\n",
    "crypto_bitcoin = crypto_with_name.loc[crypto_with_name[\"Asset_Name\"] == \"Bitcoin\"].reset_index(drop=True)\n",
    "date_range_df = pd.date_range(crypto_bitcoin[\"datetime\"].min(), crypto_bitcoin[\"datetime\"].max(), freq=\"min\")\n",
    "datetime_column = pd.DataFrame(date_range_df).rename({0: \"datetime\"}, axis=1)\n",
    "print(f\"Il manque {len(date_range_df) - len(crypto_bitcoin)} minutes dans notre dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afin de trouver les minutes manquantes, nous procédons à une comparaison\n",
    "missing_values = pd.concat([datetime_column, crypto_bitcoin[[\"datetime\"]]]).drop_duplicates(keep=False)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constate que tous les mois, on perd une minute (minuit du premier jour de chaque mois). Vérification ci-dessous pour le mois de février 2018\n",
    "crypto_bitcoin.loc[(crypto_bitcoin[\"datetime\"] > \"2018-01-31 23:57:00+00:00\") & (crypto_bitcoin[\"datetime\"] < \"2018-02-01 00:02:00+00:00\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constante aussi que le 2019-10-16, il manque des minutes et pareil à d'autres moments.\n",
    "# Que faire de ces données vides ? Dans le cas d'une minute par ci par là, on peut mettre des moyennes. Un jour complet, c'est plus périlleux.\n",
    "# Note : cas des données de ventes : si un jour manque c'est sans doute parce qu'on n'a pas eu de ventes (on met donc ventes = 0). Dans notre cas, c'est plus compliqué.\n",
    "missing_values.loc[(missing_values[\"datetime\"] > \"2019-10-16 00:00:00+00:00\") & (missing_values[\"datetime\"] < \"2019-10-17 00:00:00+00:00\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3) Question ambigüe. Maximum en moyenne ? Maximum de rendement ? Maximum à ouverture ?\n",
    "# Dans notre cas, on regarde la moyenne.\n",
    "print(f\"La valeur moyenne maximale sur une minute est atteinte le {crypto_bitcoin.loc[crypto_bitcoin['VWAP'] == crypto_bitcoin['VWAP'].max()]['datetime'].iloc[0]}.\")\n",
    "print(f\"La valeur moyenne minimale sur une minute est atteinte le {crypto_bitcoin.loc[crypto_bitcoin['VWAP'] == crypto_bitcoin['VWAP'].min()]['datetime'].iloc[0]}.\")\n",
    "# Nous savons que la valeur du bitcoin a globalement augmenté avant la crise de 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_bitcoin.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_bitcoin[\"ecart_min_max\"] = crypto_bitcoin[\"High\"] - crypto_bitcoin[\"Low\"]\n",
    "# L'écart en pourcentage doit être calculé en divisant par le min, le max ou la moyenne ? Dans notre cas, le moyenne semble le plus sensé mais débattable\n",
    "crypto_bitcoin[\"ecart_min_max_pourcent\"] = (crypto_bitcoin[\"High\"] - crypto_bitcoin[\"Low\"]) / crypto_bitcoin[\"VWAP\"] * 100\n",
    "\n",
    "print(f\"Le plus grand écart entre minimum et maximum est atteint le {crypto_bitcoin.loc[crypto_bitcoin['ecart_min_max'] == crypto_bitcoin['ecart_min_max'].max()]['datetime'].iloc[0]}.\")\n",
    "print(f\"Le plus grand écart en pourcentage entre minimum et maximum est atteint le {crypto_bitcoin.loc[crypto_bitcoin['ecart_min_max_pourcent'] == crypto_bitcoin['ecart_min_max_pourcent'].max()]['datetime'].iloc[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En terme d'écart, on constate que le maximum en écart valeur est atteint pas très loin du jour où on atteint le maximum dans l'absolu. Cela vient du faire que plus la valeur augmente, plus (logiquement) les écarts augmentent. Cette statistique n'est donc pas très utile. L'écart en pourcentage nous aide plus et nous permet de repérer des périodes d'instabilité dans le bitcoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4) Nous groupons nos données selon les différentes granularités.\n",
    "crypto_bitcoin[\"year\"] = crypto_bitcoin[\"datetime\"].dt.year\n",
    "crypto_bitcoin[\"month\"] = crypto_bitcoin[\"datetime\"].dt.month\n",
    "crypto_bitcoin[\"week_number\"] = crypto_bitcoin[\"datetime\"].apply(lambda x : x.isocalendar().week)\n",
    "crypto_bitcoin[\"week_day\"] = crypto_bitcoin[\"datetime\"].dt.weekday # Jour entre 0 et 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Que les cryptos augmentent en prix avec le nombre d'échange semble cohérent.\n",
    "crypto_bitcoin.groupby(\"year\")[\"Volume\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les mois les plus populaires sont mars, mai et Novembre...\n",
    "crypto_bitcoin.groupby(\"month\")[\"Volume\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constate que la domination de mars vient surtout de mars 2020 qui a été très fort.\n",
    "crypto_bitcoin.groupby([\"year\", \"month\"])[\"Volume\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En regardant la médiane, on se rend compte d'une réalité un peu différente (et amusante) : le trading a eu lieu en début d'année.\n",
    "crypto_bitcoin.groupby(\"month\")[\"Volume\"].median().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les semaines de l'année ne semblent pas révéler d'informations qu'on ne possède pas.\n",
    "crypto_bitcoin.groupby(\"week_number\")[\"Volume\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les jours de la semaines sont par contre très intéressants. On constate que les weekends sont derniers : les traders ne travaillent pas.\n",
    "crypto_bitcoin.groupby(\"week_day\")[\"Volume\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous voulons étudier les valeurs moyennes. Gors piège : la moyenne des moyennes n'est pas la moyenne. Il faut prendre en compte les volumes.\n",
    "\n",
    "crypto_bitcoin[\"year_week\"] = crypto_bitcoin[\"year\"].apply(str) + \" - \" + crypto_bitcoin[\"week_number\"].apply(lambda x : str(x) if len(str(x)) == 2 else '0' + str(x))\n",
    "crypto_bitcoin[\"weighted_mean\"] = crypto_bitcoin[\"VWAP\"] * crypto_bitcoin[\"Volume\"]\n",
    "mean_graph = (crypto_bitcoin.groupby([\"year_week\"])[\"weighted_mean\"].sum() / crypto_bitcoin.groupby([\"year_week\"])[\"Volume\"].sum()).reset_index().rename({0:\"weighted_mean\"}, axis=1)\n",
    "max_graph = crypto_bitcoin.groupby([\"year_week\"])[\"High\"].max().reset_index()\n",
    "min_graph = crypto_bitcoin.groupby([\"year_week\"])[\"Low\"].min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5\n",
    "# Il est possible de faire plusieurs plots (en fonction de ce que l'on souhaite présenter).\n",
    "# On décrypte la demande : prix ==> Moyenne des moyennes; max ==> max des max; min ==> min des min\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mean_graph[\"year_week\"], y=mean_graph[\"weighted_mean\"], name=f\"Prix moyen hebdomadaire\", mode=\"markers+lines\"))\n",
    "fig.add_trace(go.Scatter(x=max_graph[\"year_week\"], y=max_graph[\"High\"], name=f\"Prix maximum hebdomadaire\", mode=\"lines\"))\n",
    "fig.add_trace(go.Scatter(x=min_graph[\"year_week\"], y=min_graph[\"Low\"], name=f\"Prix minimum hebdomadaire\", mode=\"lines\"))\n",
    "fig.update_layout(title=f\"Prix moyen, min et max hebdomadaire du bitcoin\", xaxis_title=f\"Semaine de l'année\", yaxis_title=\"Prix du bitcoin\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour l'instant, on ne s'est pas intéressé au target. Voyons voir s'il a quelque chose à nous dire\n",
    "# On possède une donnée centrée (rappelez-vous, on a un log). La moyenne sur toute période de temps un tant soit peu grande sera égale à 0.\n",
    "# Qu'est-ce qu'une période grande ? Une journée = 60 * 24 points. C'est grand.\n",
    "test_graph = crypto_bitcoin.groupby([\"year_week\"])[\"Target\"].mean().reset_index()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_graph[\"year_week\"], y=test_graph[\"Target\"], name=f\"Target\", mode=\"lines\"))\n",
    "fig.update_layout(title=f\"Evolution du target pour le bitcoin\", xaxis_title=f\"Semaine de l'année\", yaxis_title=\"target\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afin de pouvoir regarder avec un peu de clarté et sans avoir à plot 1 millions de points de données, on va se focaliser sur 2020.\n",
    "graph_2020 = crypto_bitcoin.loc[crypto_bitcoin[\"year\"] == 2020]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=graph_2020[\"datetime\"], y=graph_2020[\"Target\"], name=f\"Target\", mode=\"lines\"))\n",
    "fig.update_layout(title=f\"Evolution du target pour le bitcoin sur l'année 2020\", xaxis_title=f\"datetime\", yaxis_title=\"target\")\n",
    "fig.show()\n",
    "\n",
    "# On constate qu'on possède beaucoup de \"bruits\". Avoir la valeur absolue des précédents targets parait intéressant.\n",
    "# On constate que les valeurs sont faibles ==> Faire du min max pour ramener entre 0 et 1 parait bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allons plus loin. Vous êtes data scientist, vous avez la demande et vous vous dites \"on peut faire mieux\".\n",
    "# Il existe un graphe appelé Candle sticks graph. Il nous faut le max, prix d'ouverture et de fermeture pour l'utiliser.\n",
    "# Le prix d'ouverture de la semaine est le prix d'ouverture du premier jour, le prix de fermeture est le prix de fermeture du dernier jour.\n",
    "\n",
    "opening_graph = crypto_bitcoin.loc[(crypto_bitcoin[\"week_day\"] == 0) & (crypto_bitcoin[\"datetime\"].dt.time.apply(str) == \"00:01:00\")][[\"year_week\", \"Open\"]]\n",
    "closing_graph = crypto_bitcoin.loc[(crypto_bitcoin[\"week_day\"] == 6) & (crypto_bitcoin[\"datetime\"].dt.time.apply(str) == \"23:59:00\")][[\"year_week\", \"Close\"]]\n",
    "\n",
    "fig = go.Figure(data=[go.Candlestick(x=max_graph['year_week'],\n",
    "                open=opening_graph['Open'],\n",
    "                high=max_graph['High'],\n",
    "                low=min_graph['Low'],\n",
    "                close=closing_graph['Close'])])\n",
    "fig.update_layout(title=f\"Prix d'ouverture, min et max hebdomadaire du bitcoin\", xaxis_title=f\"Semaine de l'année\", yaxis_title=\"Prix du bitcoin\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6)\n",
    "# Jettons un coup d'oeil à l'autocorrélation... On peut regarder pour les minutes, jours, semaines, mois, années...\n",
    "sm.tsa.acf(crypto_bitcoin.groupby([\"year\", \"week_number\"])[\"VWAP\"].mean().values, nlags=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 7)\n",
    "# Qu'est-ce que la tendance, la saisonnalité et le bruit ?\n",
    "# Tendance = moyenne mobile sur notre période de référence.\n",
    "# Saisonnalité = moyenne de l'écart à la tendance pendant notre saison.\n",
    "# Bruit = Différence entre valeur réelle et tendance + saisonnalité.\n",
    "# On utilise la fonction seasonal_decompose.\n",
    "\n",
    "# Attention, on choisit two_sided=True, on utilise des données de l'avenir ==> On ne peut pas utiliser notre valeur comme feature.\n",
    "seasonal_data = crypto_bitcoin.set_index(\"datetime\")\n",
    "sd = seasonal_decompose(seasonal_data[\"VWAP\"], period=24*60*7*4*2, two_sided=True) # La période proposée est de 2 mois, soit 24*60*7*4*2 minutes (points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la tendance et on la trace par rapport aux valeurs réelles.\n",
    "seasonal_data[\"observed\"] = sd.observed\n",
    "seasonal_data[\"trend\"] = sd.trend\n",
    "mean_observed = seasonal_data.reset_index().groupby(\"year_week\")[[\"observed\", \"trend\"]].mean().reset_index()\n",
    "\n",
    "# Attention, la période choisie pour faire notre tendance doit dépendre de notre problème : dans notre cas avec de grandes variations, 2 mois semble adapté.\n",
    "# Si notre but était de faire du trading au jour le jour, il aurait fallu choisir une période plus réduite sur laquelle lisser.\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mean_observed[\"year_week\"], y=mean_observed[\"observed\"], name=f\"Prix moyen hebdomadaire\", mode=\"markers+lines\"))\n",
    "fig.add_trace(go.Scatter(x=mean_observed[\"year_week\"], y=mean_observed[\"trend\"], name=f\"Tendance globale hebdomadaire\", mode=\"lines\"))\n",
    "fig.update_layout(title=f\"Prix moyen et tendance hebdomadaire du bitcoin\", xaxis_title=f\"Semaine de l'année\", yaxis_title=\"Prix du bitcoin\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde le graphe de saisonnalité dans notre cas et on ne peut pas s'empêcher d'être déçu. Les valeurs sont entre 500 et -500.\n",
    "# A l'échelle du bitcoin, c'est minuscule. Pas beaucoup d'intérêt.\n",
    "seasonal_data[\"seasonal\"] = sd.seasonal\n",
    "mean_observed_seasonal = seasonal_data.reset_index().groupby(\"year_week\")[[\"seasonal\"]].mean().reset_index()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=mean_observed_seasonal[\"year_week\"], y=mean_observed_seasonal[\"seasonal\"], name=f\"saisonnalité moyenne hebdomadaire\", mode=\"markers+lines\"))\n",
    "fig.update_layout(title=f\"Saisonnalité moyenne hebdomadaire du bitcoin\", xaxis_title=f\"Semaine de l'année\", yaxis_title=\"Valeur du bitcoin\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8 : Plusieurs éléments sont à prendre en compte.\n",
    "- Le temps est linéaire. Pour constituer nos jeux d'entrainement et de validation, il faut que les timestamp des données d'entrainement soient inférieurs à celui des jeux de validation. Typiquement dans notre cas : on prend tous le temps avant mi-2021 comme entrainement et le reste comme jeu de validation.\n",
    "- Les différences échelles d'une monnaie à l'autre sont très importantes. Il faut faire une transformatin. Notre Target semble normalisé mais dans les cas où il faut deviner un prix, on aurait plutôt cherché à prédire un pourcentage de croissance ou un truc comme ça qui exclue les échelles.\n",
    "- Il faut des échantillons de toutes les monnaies. Si ce n'est pas possible, on ne peut pas faire de prédictions.\n",
    "- Dans le cas de données périodiques, il faut prendre plusieurs périodes. Au moins 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partie 3 - Prédiction\n",
    "\n",
    "---\n",
    "\n",
    "Notre but est de prédire la VWAP avec une semaine d'avance.\n",
    "Pour se simplifier la tâche, nous allons agglomérer les données à la maille jour avec le max qui est le max de la journée, le min est le min, le volume est la somme des volumes, la VWAP qui est la VWAP pondérée...\n",
    "\n",
    "---\n",
    "\n",
    "1) Mettez en forme le jeu de données et créez la variable cible à prédire.\n",
    "1) Rajoutez la semaine de l'année en tant que variable catégorique.\n",
    "1) Afin de préparer les prédictions, normalisez les données.\n",
    "1) Ajoutez comme features un historique des valeurs sur les dernières jours et toutes autres valeurs qui semblent intéressantes. (Dans les problèmes à saisonnalité, on aurait récupéré les valeurs des précédentes saisons)\n",
    "1) En choisissant judicieusement vos jeux d'entrainement et de tests, faites des prédictions en utilisant une régression linéaire. Evaluez les résultats.\n",
    "1) Transformez la variable cible, au lieu de prédire la valeur à jour + 1, nous allons prédire la différence entre valeur actuelle et valeur à jour + 1. Comparez les résultats, conclure.\n",
    "- SARIMAX (ne fonctionne que pour les données stationnaires + ne prend pas en compte les différentes variables) ==> Inutile ici. Notre problème n'est pas stationnaire.\n",
    "- Modèles \"classiques\". Problème : on ne prédit pas très bien le rapport entre les features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1)\n",
    "\n",
    "# On commence par agréger les données à la maille jour.\n",
    "crypto_bitcoin[\"date\"] = crypto_bitcoin[\"datetime\"].dt.date\n",
    "\n",
    "opening_per_day = crypto_bitcoin.loc[(crypto_bitcoin[\"datetime\"].dt.time.apply(str) == \"00:01:00\")][[\"date\", \"Open\"]].set_index(\"date\")\n",
    "closing_per_day = crypto_bitcoin.loc[(crypto_bitcoin[\"datetime\"].dt.time.apply(str) == \"23:59:00\")][[\"date\", \"Close\"]].set_index(\"date\")\n",
    "bitcoin_per_day = crypto_bitcoin[[\"date\", \"month\", \"week_number\", \"week_day\"]].drop_duplicates([\"date\", \"month\", \"week_number\", \"week_day\"]).set_index(\"date\")\n",
    "\n",
    "df_group = crypto_bitcoin.groupby(\"date\")\n",
    "high_col = df_group[\"High\"].apply(max)\n",
    "low_col = df_group[\"Low\"].apply(min)\n",
    "vol_col = df_group[[\"Count\", \"Volume\", \"weighted_mean\"]].apply(sum)\n",
    "vol_col[\"weighted_mean\"] = vol_col[\"weighted_mean\"] / vol_col[\"Volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constitue notre base de données\n",
    "# Utiliser l'année comme variable d'apprentissage n'est pas pertinent. On va changer d'année. Les mois et semaines de l'années nous semblent pertinentd.\n",
    "\n",
    "bitcoin_per_day[\"High\"] = high_col\n",
    "bitcoin_per_day[\"Low\"] = low_col\n",
    "bitcoin_per_day[\"Open\"] = opening_per_day\n",
    "bitcoin_per_day[\"Close\"] = closing_per_day\n",
    "bitcoin_per_day[[\"Count\", \"Volume\", \"weighted_mean\"]] = vol_col\n",
    "bitcoin_per_day[\"ecart_min_max\"] = bitcoin_per_day[\"High\"] - bitcoin_per_day[\"Low\"]\n",
    "bitcoin_per_day[\"ecart_min_max_pourcent\"] = bitcoin_per_day[\"ecart_min_max\"] / bitcoin_per_day[\"weighted_mean\"]\n",
    "bitcoin_per_day = bitcoin_per_day.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie s'il nous manque des jours.\n",
    "# Dans notre cas, il ne nous manque pas de jours.\n",
    "len(pd.date_range(bitcoin_per_day[\"date\"].min(), bitcoin_per_day[\"date\"].max())) == len(bitcoin_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afin de construire notre variable cible, on applique la méthode shift qui décale nos valeurs d'un jour.\n",
    "bitcoin_per_day = bitcoin_per_day.set_index(\"date\")\n",
    "bitcoin_per_day[\"target\"] = bitcoin_per_day[\"weighted_mean\"].shift(-7)\n",
    "bitcoin_per_day[\"real_target\"] = bitcoin_per_day[\"target\"]\n",
    "bitcoin_per_day = bitcoin_per_day.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 & 3) Est-ce qu'il faut normaliser les données en utilisant uniquement les valeurs du passé ou en utilisant toutes les valeurs ?\n",
    "# Dans les cas où on possède des données avec une distribution stable, il vaut mieux utiliser toutes les valeurs. Nos données sont-elles stables ? A court terme oui. Avec plusieurs points d'inflexion.\n",
    "# On va par contre appliquer un log à nos valeurs afin de les ramener dans un écart raisonnable.\n",
    "\n",
    "col_to_log = ['High', 'Low', 'Open', 'Close', 'Count', 'Volume', 'weighted_mean', 'target']\n",
    "col_to_encode = ['month', 'week_number', 'week_day']\n",
    "\n",
    "# On applique un logarithme\n",
    "for my_col in col_to_log:\n",
    "    bitcoin_per_day[my_col] = bitcoin_per_day[my_col].apply(lambda x : np.log10(x+1))\n",
    "\n",
    "# On fait du target encoding\n",
    "for my_col in col_to_encode:\n",
    "    inter_groupby = bitcoin_per_day.groupby(my_col)\n",
    "    inter_count_target = inter_groupby[[\"target\"]].sum()\n",
    "    inter_count_target[\"target\"] = inter_count_target[\"target\"] / inter_groupby[\"target\"].count()\n",
    "    inter_count_target = inter_count_target.reset_index().rename({\"target\" : my_col + \"_te\"}, axis = 1)\n",
    "    bitcoin_per_day = pd.merge(bitcoin_per_day, inter_count_target, on=my_col)\n",
    "\n",
    "# On normalise\n",
    "# On conserve la moyenne et l'ecart type de notre variable cible.\n",
    "for my_col in bitcoin_per_day.columns:\n",
    "    if my_col not in col_to_encode + [\"date\", \"target\", \"real_target\"]:\n",
    "        bitcoin_per_day[my_col] = (bitcoin_per_day[my_col] - bitcoin_per_day[my_col].mean()) / bitcoin_per_day[my_col].std()\n",
    "    if my_col == \"target\":\n",
    "        target_mean = bitcoin_per_day[my_col].mean()\n",
    "        target_std = bitcoin_per_day[my_col].std()\n",
    "        bitcoin_per_day[my_col] = (bitcoin_per_day[my_col] - bitcoin_per_day[my_col].mean()) / bitcoin_per_day[my_col].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4)\n",
    "bitcoin_per_day = bitcoin_per_day.sort_values(\"date\").set_index(\"date\")\n",
    "bitcoin_per_day[\"last_day_value\"] = bitcoin_per_day[\"weighted_mean\"].shift(1)\n",
    "bitcoin_per_day[\"last_day_growth\"] = bitcoin_per_day[\"weighted_mean\"] - bitcoin_per_day[\"last_day_value\"]\n",
    "bitcoin_per_day[\"last_week_value\"] = bitcoin_per_day[\"weighted_mean\"].shift(7)\n",
    "bitcoin_per_day[\"last_week_growth\"] = bitcoin_per_day[\"weighted_mean\"] - bitcoin_per_day[\"last_week_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On remplit les valeurs nulles qu'on a crée de cette manière\n",
    "bitcoin_per_day = bitcoin_per_day.bfill()\n",
    "bitcoin_per_day[[\"Open\", \"Close\"]] = bitcoin_per_day[[\"Open\", \"Close\"]].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5)\n",
    "np.random.seed(42)\n",
    "\n",
    "bitcoin_ml = bitcoin_per_day.dropna()\n",
    "\n",
    "# Note : on possède des variables corrélées. On aurait sans doute du appliquer de la ridge regression ou de la lasso regression.\n",
    "# On aurait ainsi pu éliminer les variables inutiles.\n",
    "col_ml = ['High', 'Low', 'Open', 'Close',\n",
    "       'Count', 'Volume', 'weighted_mean', 'ecart_min_max',\n",
    "       'ecart_min_max_pourcent', 'month_te', 'week_number_te',\n",
    "       'week_day_te', 'last_day_value', 'last_day_growth', 'last_week_value',\n",
    "       'last_week_growth']\n",
    "\n",
    "# On récupère les valeurs de nos jeux d'entrainement et de test\n",
    "train_set = bitcoin_ml.head(4 * len(bitcoin_ml)//5)\n",
    "test_set = bitcoin_ml.tail(len(bitcoin_ml) - len(train_set))\n",
    "x_train = train_set[col_ml]\n",
    "y_train = train_set[\"target\"]\n",
    "x_test = test_set[col_ml]\n",
    "y_test = test_set[[\"real_target\"]].reset_index(drop=True)\n",
    "\n",
    "# On crée notre modèle de ML\n",
    "my_reg = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On observe les performances de notre modèle.\n",
    "# Afin de voir s'il s'en sort bien, on le compare à l'erreur qu'on aurait eu si on avait dit \"la nouvelle valeur = ancienne valeur + croissance depuis la semaine dernière\"\n",
    "test_prediction = 10**(my_reg.predict(x_test) * target_std + target_mean)\n",
    "dumb_estimator = pd.DataFrame(10**((test_set[\"weighted_mean\"] + test_set[\"last_week_growth\"])* target_std + target_mean)).reset_index(drop=True).rename({0 : \"dumb_estimator\"}, axis = 1)\n",
    "y_test[\"AbsDifference_ml\"] = (y_test[\"real_target\"] - pd.DataFrame(test_prediction, columns = [\"real_target\"])[\"real_target\"]).apply(abs)\n",
    "y_test[\"AbsDifference_estim\"] = (y_test[\"real_target\"] - dumb_estimator[\"dumb_estimator\"]).apply(abs)\n",
    "\n",
    "# Notre modèle de ml fait mieux que notre estimateur mais c'est pas flagrant.\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(x=y_test[\"AbsDifference_ml\"], name=\"Erreur pour ml\"))\n",
    "fig.add_trace(go.Box(x=y_test[\"AbsDifference_estim\"], name=\"Erreur pour estimateur\"))\n",
    "fig.update_layout(title=f\"Comparaison des valeurs d'erreurs en fonction du modèle de prédiction\", xaxis_title=f\"Valeur d'erreur\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison visuel du prix du bitcoin vs notre estimateur\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_set.index, y=y_test[\"real_target\"], name=f\"Prix réel du bitcoin\", mode=\"markers+lines\"))\n",
    "fig.add_trace(go.Scatter(x=test_set.index, y=pd.DataFrame(test_prediction, columns = [\"real_target\"])[\"real_target\"], name=f\"Prédiction ML\", mode=\"lines\"))\n",
    "fig.add_trace(go.Scatter(x=test_set.index, y=dumb_estimator[\"dumb_estimator\"], name=f\"Prédiction estimateur\", mode=\"lines\"))\n",
    "fig.update_layout(title=f\"Evolution du prix du bitcoin au cours du temps\", xaxis_title=f\"Date\", yaxis_title=\"Prix du bitcoin\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6) On refait un peu tout depuis la question 1\n",
    "\n",
    "# On commence par agréger les données à la maille jour.\n",
    "crypto_bitcoin[\"date\"] = crypto_bitcoin[\"datetime\"].dt.date\n",
    "\n",
    "opening_per_day = crypto_bitcoin.loc[(crypto_bitcoin[\"datetime\"].dt.time.apply(str) == \"00:01:00\")][[\"date\", \"Open\"]].set_index(\"date\")\n",
    "closing_per_day = crypto_bitcoin.loc[(crypto_bitcoin[\"datetime\"].dt.time.apply(str) == \"23:59:00\")][[\"date\", \"Close\"]].set_index(\"date\")\n",
    "bitcoin_per_day = crypto_bitcoin[[\"date\", \"month\", \"week_number\", \"week_day\"]].drop_duplicates([\"date\", \"month\", \"week_number\", \"week_day\"]).set_index(\"date\")\n",
    "\n",
    "df_group = crypto_bitcoin.groupby(\"date\")\n",
    "high_col = df_group[\"High\"].apply(max)\n",
    "low_col = df_group[\"Low\"].apply(min)\n",
    "vol_col = df_group[[\"Count\", \"Volume\", \"weighted_mean\"]].apply(sum)\n",
    "vol_col[\"weighted_mean\"] = vol_col[\"weighted_mean\"] / vol_col[\"Volume\"]\n",
    "\n",
    "# On constitue notre base de données\n",
    "# Utiliser l'année comme variable d'apprentissage n'est pas pertinent. On va changer d'année. Les mois et semaines de l'années nous semblent pertinentd.\n",
    "\n",
    "bitcoin_per_day[\"High\"] = high_col\n",
    "bitcoin_per_day[\"Low\"] = low_col\n",
    "bitcoin_per_day[\"Open\"] = opening_per_day\n",
    "bitcoin_per_day[\"Close\"] = closing_per_day\n",
    "bitcoin_per_day[[\"Count\", \"Volume\", \"weighted_mean\"]] = vol_col\n",
    "bitcoin_per_day[\"ecart_min_max\"] = bitcoin_per_day[\"High\"] - bitcoin_per_day[\"Low\"]\n",
    "bitcoin_per_day[\"ecart_min_max_pourcent\"] = bitcoin_per_day[\"ecart_min_max\"] / bitcoin_per_day[\"weighted_mean\"]\n",
    "bitcoin_per_day = bitcoin_per_day.reset_index()\n",
    "\n",
    "# Afin de construire notre variable cible, on applique la méthode shift qui décale nos valeurs d'un jour.\n",
    "bitcoin_per_day = bitcoin_per_day.set_index(\"date\")\n",
    "bitcoin_per_day[\"target\"] = bitcoin_per_day[\"weighted_mean\"].shift(-7)\n",
    "bitcoin_per_day[\"real_target\"] = bitcoin_per_day[\"target\"] \n",
    "bitcoin_per_day[\"target\"] = (bitcoin_per_day[\"target\"] - vol_col[\"weighted_mean\"]) / vol_col[\"weighted_mean\"]\n",
    "bitcoin_per_day[\"real_weighted_mean\"] = vol_col[\"weighted_mean\"]\n",
    "bitcoin_per_day = bitcoin_per_day.reset_index()\n",
    "\n",
    "# On va par contre appliquer un log à nos valeurs afin de les ramener dans un écart raisonnable.\n",
    "\n",
    "col_to_log = ['High', 'Low', 'Open', 'Close', 'Count', 'Volume', 'weighted_mean']\n",
    "col_to_encode = ['month', 'week_number', 'week_day']\n",
    "\n",
    "# On applique un logarithme\n",
    "for my_col in col_to_log:\n",
    "    bitcoin_per_day[my_col] = bitcoin_per_day[my_col].apply(lambda x : np.log10(x+1))\n",
    "\n",
    "# On fait du target encoding\n",
    "for my_col in col_to_encode:\n",
    "    inter_groupby = bitcoin_per_day.groupby(my_col)\n",
    "    inter_count_target = inter_groupby[[\"target\"]].sum()\n",
    "    inter_count_target[\"target\"] = inter_count_target[\"target\"] / inter_groupby[\"target\"].count()\n",
    "    inter_count_target = inter_count_target.reset_index().rename({\"target\" : my_col + \"_te\"}, axis = 1)\n",
    "    bitcoin_per_day = pd.merge(bitcoin_per_day, inter_count_target, on=my_col)\n",
    "\n",
    "# On normalise\n",
    "# On conserve la moyenne et l'ecart type de notre variable cible.\n",
    "for my_col in bitcoin_per_day.columns:\n",
    "    if my_col not in col_to_encode + [\"date\", \"target\", \"real_target\", \"real_weighted_mean\"]:\n",
    "        bitcoin_per_day[my_col] = (bitcoin_per_day[my_col] - bitcoin_per_day[my_col].mean()) / bitcoin_per_day[my_col].std()\n",
    "    if my_col == \"target\":\n",
    "        target_mean = bitcoin_per_day[my_col].mean()\n",
    "        target_std = bitcoin_per_day[my_col].std()\n",
    "        bitcoin_per_day[my_col] = (bitcoin_per_day[my_col] - bitcoin_per_day[my_col].mean()) / bitcoin_per_day[my_col].std()\n",
    "\n",
    "bitcoin_per_day = bitcoin_per_day.sort_values(\"date\").set_index(\"date\")\n",
    "bitcoin_per_day[\"last_day_value\"] = bitcoin_per_day[\"weighted_mean\"].shift(1)\n",
    "bitcoin_per_day[\"last_day_growth\"] = (bitcoin_per_day[\"weighted_mean\"] - bitcoin_per_day[\"last_day_value\"]) / bitcoin_per_day[\"weighted_mean\"]\n",
    "bitcoin_per_day[\"last_week_value\"] = bitcoin_per_day[\"weighted_mean\"].shift(7)\n",
    "bitcoin_per_day[\"last_week_growth\"] = (bitcoin_per_day[\"weighted_mean\"] - bitcoin_per_day[\"last_week_value\"])/bitcoin_per_day[\"weighted_mean\"]\n",
    "\n",
    "# On remplit les valeurs nulles qu'on a crée de cette manière\n",
    "bitcoin_per_day = bitcoin_per_day.bfill()\n",
    "bitcoin_per_day[[\"Open\", \"Close\"]] = bitcoin_per_day[[\"Open\", \"Close\"]].ffill()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "bitcoin_ml = bitcoin_per_day.dropna()\n",
    "\n",
    "# Note : on possède des variables corrélées. On aurait sans doute du appliquer de la ridge regression ou de la lasso regression.\n",
    "# On aurait ainsi pu éliminer les variables inutiles.\n",
    "col_ml = ['High', 'Low', 'Open', 'Close',\n",
    "       'Count', 'Volume', 'weighted_mean', 'ecart_min_max',\n",
    "       'ecart_min_max_pourcent', 'month_te', 'week_number_te',\n",
    "       'week_day_te', 'last_day_value', 'last_day_growth', 'last_week_value',\n",
    "       'last_week_growth']\n",
    "\n",
    "# On récupère les valeurs de nos jeux d'entrainement et de test\n",
    "train_set = bitcoin_ml.head(4 * len(bitcoin_ml)//5)\n",
    "test_set = bitcoin_ml.tail(len(bitcoin_ml) - len(train_set))\n",
    "x_train = train_set[col_ml]\n",
    "y_train = train_set[\"target\"]\n",
    "x_test = test_set[col_ml]\n",
    "y_test = test_set[[\"real_target\", \"real_weighted_mean\"]].reset_index(drop=True)\n",
    "\n",
    "# On crée notre modèle de ML\n",
    "my_reg = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On observe les performances de notre modèle.\n",
    "# On constate que l'erreur médiane est inférieure à précédemment (youhou !)\n",
    "test_prediction = my_reg.predict(x_test) * target_std + target_mean\n",
    "y_test[\"AbsDifference_ml\"] = (y_test[\"real_target\"] - (test_prediction * y_test[\"real_weighted_mean\"] + y_test[\"real_weighted_mean\"])).apply(abs)\n",
    "\n",
    "# Notre modèle de ml fait mieux que notre estimateur mais c'est pas flagrant.\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(x=y_test[\"AbsDifference_ml\"], name=\"Erreur pour ml\"))\n",
    "fig.update_layout(title=f\"Comparaison des valeurs d'erreurs en fonction du modèle de prédiction\", xaxis_title=f\"Valeur d'erreur\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison visuel du prix du bitcoin vs notre estimateur\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_set.index, y=y_test[\"real_target\"], name=f\"Prix réel du bitcoin\", mode=\"markers+lines\"))\n",
    "fig.add_trace(go.Scatter(x=test_set.index, y=test_prediction * y_test[\"real_weighted_mean\"] + y_test[\"real_weighted_mean\"], name=f\"Prédiction ML\", mode=\"lines\"))\n",
    "fig.update_layout(title=f\"Evolution du prix du bitcoin au cours du temps\", xaxis_title=f\"Date\", yaxis_title=\"Prix du bitcoin\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En regardant les figures, on constate que le deuxième modèle se débrouille mieux que le premier en moyenne (il est plus simple de deviner un pourcentage entre -50 et 50 qu'une valeur potentiellement très élevée de croissance). Néanmoins, on possède un modèle qui \"triche\" : il prend plus ou moins la même valeur que la veille ! C'est malin mais c'est pas fou.\n",
    "\n",
    "On demeure cependant insatisfait du résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
