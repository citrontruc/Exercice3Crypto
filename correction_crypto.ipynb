{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3\n",
    "\n",
    "---\n",
    "\n",
    "Le but de ce notebook est d'étudier le jeu de données de la valeur des crypto afin de pouvoir faire une prédiction des gains possibles qu'on pourrait avoir.\n",
    "\n",
    "Pour des raisons de taille, le jeu de données n'a pas pu être uploadé avec le github. Il faudra donc le récupérer à [l'adresse suivante](https://www.kaggle.com/c/g-research-crypto-forecasting/data). Seul les jeux de données train.csv et asset_details.csv nous intéressent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import dateutil.tz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des tables dans l'espace de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_train = pd.read_csv(\"g-research-crypto-forecasting/train.csv\")\n",
    "# supplemental_train est une donnée d'entrainement fournie pour indication. Ne pas utiliser.\n",
    "#crypto_additional_train = pd.read_csv(\"g-research-crypto-forecasting/supplemental_train.csv\")\n",
    "crypto_info = pd.read_csv(\"g-research-crypto-forecasting/asset_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piège : la colonne timestamp possède une valeur assez peu lisible.\n",
    "crypto_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 - Qualité des données\n",
    "\n",
    "1) Trouvez le nombre de lignes et de colonnes.\n",
    "1) Faites un join avec la table des infos pour restituer à chaque cryptomonnaie son nom.\n",
    "1) Trouvez le nombre de valeur manquantes. Quelle est la crypto-monnaie avec le plus de valeurs manquantes ?\n",
    "1) Créer une colonne date qui change la colonne timestamp en une vraie date.\n",
    "1) Timestamp maximal, minimal et nombre de points pour chacun de nos assets. Quelle semble être la granularité de notre timeseries ?\n",
    "1) Pour chacune des crypto-monnaies, de combien de points disposons-nous ? Que pouvons-nous dire en comparant avec la date minimal pour chacune de nos cryptomonnaies ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1)\n",
    "print(f\"Nous possédons {len(crypto_train)} lignes de données et {len(crypto_train.columns)} colonnes. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2) On fusionne les deux tables. Dans notre cas, on fait un left join sur la colonne commune.\n",
    "crypto_with_name = pd.merge(crypto_train, crypto_info, on=\"Asset_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3) trouvons le nombre de valeurs manquantes.\n",
    "\n",
    "# Il est déconseillé d'utiliser la commande .describe() car nous allons alors effectuer des calculs de moyennes et autres variables.\n",
    "# Dans notre cas, notre dataset est trop grand pour calculer tout ça.\n",
    "# crypto_train.describe()\n",
    "\n",
    "# A la place, nous allons nous borner à faire un calcul simple. Par colonne, nous disposons du nombre suivant de valeurs nulles.\n",
    "crypto_with_name.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constate que la majorité des valeurs nulles proviennent de la colonne target. Dans le cas des Timestamp, avoir des valeurs nulles est très grave.\n",
    "# En effet, on coupe la continuité du temps et comme le rapport de chaque variable avec les précédentes est important, on se prive d'une information capitale.\n",
    "\n",
    "# Nous séparons le nombre de valeurs nulles par crypto\n",
    "crypto_with_name.loc[crypto_with_name[\"Target\"].isna()].groupby(\"Asset_Name\").count()[\"timestamp\"].sort_values(ascending=False)\n",
    "# Le fait que les monnaies les plus connues soient les plus documentées nous semble pertinent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4) On utilise la librairie datetime qui possède une bibliothèque associée.\n",
    "tz_keep = dateutil.tz.tzutc()\n",
    "crypto_with_name[\"datetime\"] = crypto_with_name[\"timestamp\"].apply(lambda x : datetime.fromtimestamp(x, tz=tz_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5) On peut appliquer les méthodes min et max aux dates ==> Pas de soucis.\n",
    "print(f\"Les extremums des dates sont {crypto_with_name['datetime'].min()} pour le minimum et {crypto_with_name['datetime'].max()} pour le maximum.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En regardant les valeurs classées, on devine que la granularité des données est une minute.\n",
    "sorted(crypto_with_name['datetime'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6) On compte le nombre de points de données que l'on possède pour chacune de nos cryptomonnaies.\n",
    "crypto_with_name.groupby('Asset_Name')['timestamp'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On possède beaucoup de valeurs depuis le premier janvier 2018 MAIS pour ces monnaies, on ne possède pas le même nombre de points.\n",
    "# Il doit manquer des points intermédiaires ==> Ouch\n",
    "crypto_with_name.groupby('Asset_Name')['datetime'].min().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On possède toutes les monnaies jusqu'au 21 septembre 2021. On va effectivement avoir des valeurs manquantes au milieu.\n",
    "crypto_with_name.groupby('Asset_Name')['datetime'].max().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_with_name.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 - Analyse données\n",
    "\n",
    "---\n",
    "\n",
    "## Explication des valeurs des colonnes.\n",
    "\n",
    "- **timestamp** : La minute considérée par la colonne.\n",
    "- **Asset_ID** & **Asset_Name** : identifiant de la crypto-monnaie.\n",
    "- **Count** : Nombre d'échanges de la monnaie lors de la minute.\n",
    "- **Open** & **Close** : Prix d'ouverture et de fermeture (prix au début et à la fin de la minute).\n",
    "- **High** & **Low** : Prix maximal et minimal atteint dans la minute.\n",
    "- **Volume** : Montant total échangé lors de la minute.\n",
    "- **VWAP** : Prix moyen sur la minute pondéré par volume de vente.\n",
    "- **Target** : Log du retour sur investissement sur les 15 prochaines minutes auquel on a appliqué une transformation. f(log(prix à l'instant t+16) - log(prix à l'instant t+1))\n",
    "- **Weight** : Lié à la cryptomonnaie. Importance de la crypto-monnaie (valeur arbitraire).\n",
    "\n",
    "---\n",
    "\n",
    "Nous n'allons pas utiliser les valeurs de weight et target. Nous allons essayer de prédire la VWAP de la monnaie avec 7j d'avance.\n",
    "\n",
    "---\n",
    "\n",
    "1) Calculez la corrélations entre les valeurs moyennes des différentes monnaies sur l'année 2021. Ce résultat vous étonne-t-il ?\n",
    "1) Au vue de la question précédente, nous allons nous intéresser uniquement au bitcoin pour l'instant. Identifiez les timestamp manquantes pour le bitcoin.\n",
    "1) Quand sont atteints les optimums et pourquoi ? Quand sont atteint les plus grands écarts min max en valeur puis en %\n",
    "1) Quels jours de la semaine, quels mois de l'année et quelles semaines de l'années y a-t-il le plus de transactions (en moyenne) ? Décomposez les statistiques selon les périodes qui vous semblent pertinentes.\n",
    "1) Faire un graphe qui présente par semaine le prix moyen des cryptomonnaies avec leur min et le max (dans le même graphe).\n",
    "1) Autocorrelation décalée pour voir s'il y a des périodicités.\n",
    "1) Faire des graphe qui présentent la tendance et  la saisonnalité des données.\n",
    "1) Quelle méthode utiliseriez-vous pour faire jeux de test et train ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partie 3 - Prédiction\n",
    "\n",
    "---\n",
    "\n",
    "Notre but est de prédire la VWAP avec une semaine d'avance.\n",
    "Pour se simplifier la tâche, nous allons agglomérer les données à la maille jour avec le max qui est le max de la journée, le min est le min, le volume est la somme des volumes, la VWAP qui est la VWAP pondérée...\n",
    "\n",
    "---\n",
    "\n",
    "1) Mettez en forme le jeu de données et créez la variable cible à prédire.\n",
    "1) Rajoutez la semaine de l'année en tant que variable catégorique.\n",
    "1) Afin de préparer les prédictions, normalisez les données.\n",
    "1) Ajoutez comme features un historique des valeurs sur les dernières jours et toutes autres valeurs qui semblent intéressantes. (Dans les problèmes à saisonnalité, on aurait récupéré les valeurs des précédentes saisons)\n",
    "1) En choisissant judicieusement vos jeux d'entrainement et de tests, faites des prédictions en utilisant une régression linéaire. Evaluez les résultats.\n",
    "1) Transformez la variable cible, au lieu de prédire la valeur à jour + 1, nous allons prédire la différence entre valeur actuelle et valeur à jour + 1. Comparez les résultats, conclure.\n",
    "- SARIMAX (ne fonctionne que pour les données stationnaires + ne prend pas en compte les différentes variables) ==> Inutile ici. Notre problème n'est pas stationnaire.\n",
    "- Modèles \"classiques\". Problème : on ne prédit pas très bien le rapport entre les features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
